{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df9bb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/rohan-paul/Gravitational-Wave-Detection_Kaggle_Competition/blob/main/Kaggle_NBs/1_TimeSeries_GWPy_Data_Preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bf36170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000029F7A2DC880>, 'Connection to es001-surf.zone2.proxy.allianz timed out. (connect timeout=15)')': /simple/gwpy/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000029F7A2DCCA0>, 'Connection to es001-surf.zone2.proxy.allianz timed out. (connect timeout=15)')': /simple/gwpy/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000029F7A2DCDC0>, 'Connection to es001-surf.zone2.proxy.allianz timed out. (connect timeout=15)')': /simple/gwpy/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000029F7A2DCF10>, 'Connection to es001-surf.zone2.proxy.allianz timed out. (connect timeout=15)')': /simple/gwpy/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000029F7A2FF100>, 'Connection to es001-surf.zone2.proxy.allianz timed out. (connect timeout=15)')': /simple/gwpy/\n",
      "ERROR: Could not find a version that satisfies the requirement gwpy (from versions: none)\n",
      "ERROR: No matching distribution found for gwpy\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge gwpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2fde9f",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0a30c00",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gwpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m signal\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgwpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtimeseries\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimeSeries\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgwpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Plot\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gwpy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from gwpy.timeseries import TimeSeries\n",
    "from gwpy.plot import Plot\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from colorama import Fore, Back, Style\n",
    "plt.style.use('ggplot')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPool1D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "import torch\n",
    "from nnAudio.Spectrogram import CQT1992v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f76943b",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1fdba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv(\"data/training_labels.csv\")\n",
    "train_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904f4b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the file file path from all 4-labels of nested folder structure\n",
    "files_paths = glob(root_dir + '/train/*/*/*/*')\n",
    "''' The glob module finds all the pathnames matching a specified pattern according to the rules \n",
    "used by the Unix shell, although results are returned in arbitrary order. \n",
    "No tilde expansion is done, but *, ?, and character ranges expressed with [] will be correctly matched. \n",
    "We can use glob to search for a specific file pattern, or perhaps more usefully, search for files where the \n",
    "filename matches a certain pattern by using wildcard characters.\n",
    "\n",
    "'''\n",
    "\n",
    "# get the list of ids from the .npy files\n",
    "ids_from_npy_files = [path.split(\"/\")[-1].split(\".\")[0] for path in files_paths]\n",
    "# [-1] means the last element in a sequence,\n",
    "# print(ids_from_npy_files)\n",
    "\n",
    "# get a dataframe with paths and ids of those .npy files\n",
    "df_path_id = pd.DataFrame({'path': files_paths, 'id':ids_from_npy_files})\n",
    "df_path_id.head()\n",
    "\n",
    "# merging that above df with the target\n",
    "df_train = pd.merge(left=train_labels, right=df_path_id, on='id')\n",
    "display(df_train.head())\n",
    "\n",
    "# verifying the shape of the merged df has 5,60,000 rows and 3 columns\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2936dcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify the the 2 classes of targets of 1 and 0\n",
    "target_1_df_train = df_train[df_train.target == 1]\n",
    "target_0_df_train = df_train[df_train.target == 0]\n",
    "print(\"Class distribution of Target: \\n \", train_labels.target.value_counts())\n",
    "display(target_1_df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17202dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = 'target' , data=train_labels)\n",
    "plt.title('Target Class Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349c92ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" First, we define the constructor to initialize the configuration of the generator.\n",
    "Note that here, we assume the path to the data is in a dataframe column.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "\n",
    "    # For this dataset the list_IDs are the value of the ids\n",
    "    # for each of the time-series file\n",
    "    # i.e. for Train data => values of column 'id' from training_labels.csv\n",
    "\n",
    "    # Also Note we have earlier defined our labels to be the below\n",
    "    # labels = pd.read_csv(root_dir + \"training_labels.csv\")\n",
    "    # and the argument \"data\" is that label here.\n",
    "    def __init__(self, path, list_IDs, data, batch_size):\n",
    "        self.path = path\n",
    "        self.list_IDs = list_IDs\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "\n",
    "    \"\"\" __len__ essentially returns the number of steps in an epoch, using the samples and the batch size.\n",
    "        Each call requests a batch index between 0 and the total number of batches, where the latter is specified in the __len__ method.\n",
    "        A common practice is to set this value to (samples / batch size)\n",
    "        so that the model sees the training samples at most once per epoch.\n",
    "        Now, when the batch corresponding to a given index is called, the generator executes the __getitem__ method to generate it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __len__(self):\n",
    "        len_ = int(len(self.list_IDs)/self.batch_size)\n",
    "        if len_ * self.batch_size < len(self.list_IDs):\n",
    "            len_ += 1\n",
    "        return len_\n",
    "\n",
    "    \"\"\"  __getitem__ method is called with the batch number as an argument to obtain a given batch of data.\n",
    "\n",
    "    \"\"\"\n",
    "    def __getitem__(self, index):\n",
    "        # get the range to to feed to keras for each epoch\n",
    "        # incrementing by +1 the bath_size\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        return X, y\n",
    "\n",
    "    \"\"\" And finally the core method which will actually produce batches of data. This private method __data_generation \"\"\"\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        # We have 5,60,000 files, each with dimension of 3 * 4096\n",
    "        X = np.zeros((self.batch_size, 3, 4096))\n",
    "        y = np.zeros((self.batch_size, 1))\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            id_ = self.data.loc[ID, \"id\"]\n",
    "            file = id_ + \".npy\"  # build the file name\n",
    "            path_in = \"/\".join([self.path, id_[0], id_[1], id_[2]]) + \"/\"\n",
    "            # there are three nesting labels inside train/ or test/\n",
    "            data_array = np.load(path_in + file)            \n",
    "            data_array = (data_array - data_array.mean())/data_array.std()\n",
    "            X[i, ] = data_array\n",
    "            y[i, ] = self.data.loc[ID, 'target']\n",
    "        # print(X)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca399c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(root_dir +  'sample_submission.csv')\n",
    "# print(len(train_labels)) # 5,60,000\n",
    "# print(len(sample_submission)) # 2,26,000\n",
    "train_ids = train_labels['id'].values\n",
    "# train_ids # ['00000e74ad', '00001f4945', '0000661522' ... ]\n",
    "y = train_labels['target'].values\n",
    "test_ids = sample_submission['id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f74e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_labels = pd.read_csv(root_dir + \"training_labels.csv\", nrows=1000)\n",
    "\n",
    "# ********************\n",
    "\n",
    "# Now I shall genereate train indices, validation indices and test indices\n",
    "# Which are just the values from the 0-based indices\n",
    "train_indices, validation_indices = train_test_split(list(train_labels.index), test_size=0.33, random_state=2021)\n",
    "# print(len(train_indices))\n",
    "print(len(validation_indices))\n",
    "test_indices = list(sample_submission.index)\n",
    "# test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf89bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator_for_seq_model = DataGenerator( root_dir +  'train/', train_indices, train_labels, 64)\n",
    "# print(train_generator_for_seq_model)\n",
    "validation_generator_for_seq_model = DataGenerator( root_dir + 'train/', validation_indices, train_labels, 64)\n",
    "test_generator_for_seq_model = DataGenerator( root_dir + 'test/', test_indices, sample_submission, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b607ea",
   "metadata": {},
   "source": [
    "https://github.com/PraveenThakkannavar/G2Net-Gravitational-Wave-Detection/blob/main/SIMPLE_CNN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c492194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "import librosa.display\n",
    "import torch\n",
    "\n",
    "# this is used for Contant Q Transform\n",
    "from nnAudio.Spectrogram import CQT1992v2\n",
    "from tensorflow.keras.applications import EfficientNetB0 as efn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
