{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b50db70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be46a83e",
   "metadata": {},
   "source": [
    "https://github.com/PraveenThakkannavar/G2Net-Gravitational-Wave-Detection/blob/main/KERAS.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c1635d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gwpy.timeseries import TimeSeries # time domain data array in gwpy\n",
    "from gwpy.plot import Plot # plotting in gwpy\n",
    "from scipy import signal # for signal processing\n",
    "from sklearn.preprocessing import MinMaxScaler # for preprocessing\n",
    "from scipy.signal import hann\n",
    "from src.load.get_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fbef23",
   "metadata": {},
   "source": [
    "Defining a function to read numpy data and convert into GWpy TimeSeries format and plot the data. All the files are 2s recordings at 2048Hz sample rate from the 3 detectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f90407e",
   "metadata": {},
   "source": [
    "PRE-PROCESSING STEPS:\n",
    "General pre-processing are: Referred Link: https://iopscience.iop.org/article/10.1088/1361-6382/ab685e\n",
    "Applying a window function (Tukey - tapered cosine window) to suppress spectral leakage\n",
    "Whitening the spectrum\n",
    "Bandpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1579b097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7e4b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cee00c27",
   "metadata": {},
   "source": [
    "## Reading the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f921c865",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels=pd.read_csv(\"data/training_labels.csv\")\n",
    "training_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceb7251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3e20763",
   "metadata": {},
   "source": [
    "### Signal Transformations - Spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ce8ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a random sample from the train data\n",
    "sample_gw_id = training_labels[training_labels['target'] == 1].sample(random_state=42)['id'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c51b5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_sample_spectogram(training_labels,sample_gw_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9578fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw another random sample from train without gravitational wave signal\n",
    "sample_no_gw_id = training_labels[training_labels['target'] == 0].sample(random_state=42)['id'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf67d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the sample without gravitational wave signal\n",
    "visualize_sample_spectogram(train_data, sample_no_gw_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855da538",
   "metadata": {},
   "source": [
    "### Signal Transformations- MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec91c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_sample_mfcc(training_labels,sample_gw_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cda234",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_sample_mfcc(train_data, sample_no_gw_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cfaec0",
   "metadata": {},
   "source": [
    "### Signal Treansformations- CQT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93e3672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot two spectrograms for sample w/ and w/o GW signal side-by-side\n",
    "plot_q_transform_sbs(sample_gw_id, sample_no_gw_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c4309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the sample with obvious \"chirp\"\n",
    "# id from: https://www.kaggle.com/mistag/data-preprocessing-with-gwpy\n",
    "plot_q_transform('0021f9dd71')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bddc74",
   "metadata": {},
   "source": [
    "As seen in the image below, from the publication, our end signal looks very different to one the from the paper. \n",
    "\n",
    "To this end, let's see if we can get any info from the spectrogram images by tranforming the data using Constant Q-Transform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae593975",
   "metadata": {},
   "source": [
    "Constant Q-Transform The signal analysis didn't provide much insights, so let's try the second method in signal processing. Tranforming the waves into spectrograms images, i.e. frequency-domain, and then visualize them. This technique is widely used in audio analysis (as shown here on this TensorFlow tutorial) and since our data is a wave with bunch of frequencies, we can use the same technique as well.\n",
    "\n",
    "The advantage of using a spectrogram, over a direct Fourier Transform where you lose time info, is that it captures the shift or change in frequencies over time and this removes white noise frequencies that are persistent, leaving the signals of interest. Constant Q-Transform is one way to visualize the spectrogram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3c29f5",
   "metadata": {},
   "source": [
    "Visibly, all three signals have different features and the above were plotted from a sample which has gravitational waves, and it shows the famous 'chirp' confirming the presence of gravitational waves. This transformation removes the unwanted noise frequencies, but still some of it remains, but a signal has to be detected in all three waves to be predicted as gravitational wave.\n",
    "\n",
    "Next, we can compare how the Q-Transforms look for samples with and without gravitational wave signals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edb69a6",
   "metadata": {},
   "source": [
    "This clearly visible \"chirp\" can be seen with minimal background noise and is one of the good samples where the signal-to-ratio is quite strong, but as mentioned not all samples are like this and thus we use ML models to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41f3107",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/SiddharthPatel45/gravitational-wave-detection/blob/main/code/gw-detection-analysis.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33425485",
   "metadata": {},
   "source": [
    "Typical signal processing workflow\n",
    "Next, we try to implement the steps from this paper referenced above by following these steps:\n",
    "\n",
    "Plot the raw signal\n",
    "Window the signal\n",
    "Whiten the signal\n",
    "Bandpass the signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b346044",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3481e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the raw signal\n",
    "sample_gw_ts = TimeSeries(get_data(sample_gw_id)[0], sample_rate=sample_rate)\n",
    "plot = sample_gw_ts.plot()\n",
    "ax = plot.gca()\n",
    "ax.set_xlim(0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab40a7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a window of lenght of the signal\n",
    "hann_win = hann(sample_rate*signal_length, False)\n",
    "plt.plot(hann_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9658b163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the windowed signal\n",
    "sample_gw_ts_win = sample_gw_ts * hann_win\n",
    "plot = sample_gw_ts_win.plot()\n",
    "ax = plot.gca()\n",
    "ax.set_xlim(0, 2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d8dc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = sample_gw_ts_win.asd(fftlength=2).plot(figsize=[12, 6])\n",
    "plt.xlim(10,1024)\n",
    "plt.ylim(1e-25, 1e-20);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45236e2",
   "metadata": {},
   "source": [
    "This is the windowed signal. Next, let's plot a whitened signal. As mentioned in the tutorial we referenced earlier, whitening the data is suppressing the extra noise at low frequencies and at the spectral lines, to better see the weak signals in the most sensitive band. Whitening is always one of the first steps in astrophysical data analysis (searches, parameter estimation). Whitening requires no prior knowledge of spectral lines, etc; only the data are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6589fda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the whitened signal\n",
    "plot = sample_gw_ts.whiten().plot()\n",
    "ax = plot.gca()\n",
    "ax.set_xlim(0, 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48437f4a",
   "metadata": {},
   "source": [
    "This is the whitened signal. Next, since we know this data is from merger binary black holes, the frequency is in lower range and this we apply a bandpass filter to passthrough signals between 35 ~ 350 Hz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b295416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bandpass the above whitened data and plot\n",
    "plot = sample_gw_ts.whiten().bandpass(35, 350).plot()\n",
    "ax = plot.gca()\n",
    "ax.set_xlim(0, 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cd0b9c",
   "metadata": {},
   "source": [
    "Q-Transform\n",
    "The constant quality factor transform (CQT), introduced by J.C. Brown in 1988, is an interesting alternative to the windowed Fourier transform (STFT / Short Time Fourier Transform) or wavelets, for time-frequency analysis.\n",
    "\n",
    "The constant-Q transform transforms a data series to the frequency domain. It is related to the Fourier transform. In general, the transform is well suited to musical data and proves useful where frequencies span several octaves.It is more useful in the identification of instruments.\n",
    "\n",
    "Unlike the Fourier transform, but similar to the mel scale, the constant-Q transform (Wikipedia) uses a logarithmically spaced frequency axis. The original paper reference below\n",
    "\n",
    "Judith C. Brown, \"Calculation of a constant Q spectral transform,\" J. Acoust. Soc. Am., 89(1):425–434, 1991.\n",
    "\n",
    "From Wikipedia - In mathematics and signal processing, the constant-Q transform, simply known as CQT transforms a data series to the frequency domain. It is related to the Fourier transform[1] and very closely related to the complex Morlet wavelet transform. In general, the transform is well suited to musical data, and this can be seen in some of its advantages compared to the fast Fourier transform. As the output of the transform is effectively amplitude/phase against log frequency, fewer frequency bins are required to cover a given range effectively, and this proves useful where frequencies span several octaves. As the range of human hearing covers approximately ten octaves from 20 Hz to around 20 kHz, this reduction in output data is significant.\n",
    "\n",
    "A Constant Q transform is a variation on the Discrete Fourier Transform (DFT). In other words, it is a type of wavelet transform.\n",
    "\n",
    "I only have a casual understanding of both types of transforms myself, so take what I'm saying with a grain of salt.\n",
    "\n",
    "A standard DFT uses a constant window size throughout all frequencies. This typically leads to a pretty consistent, fully continuous transform. However, the constant bin size for all frequencies leads to some problems when you map frequency on a logarithmic scale. Specifically, peaks on the lower end are incredibly wide (sometimes up to half an octave), lacking any sort of detail.\n",
    "\n",
    "This is an issue for emulating human perception because humans perceive frequency on a logarithmic scale.\n",
    "\n",
    "A Constant Q transform seeks to solve this problem by increasing the window size for lower frequencies, and alleviate some of the computational strain caused by this by reducing the window size used for high frequencies. It's pretty effective at this, but has a few drawbacks.\n",
    "\n",
    "The computational complexity of a Constant Q transform is only slightly larger than that of a standard DFT, but because the window size changes per frequency, it is impossible to apply the typical optimizations of the FFT to a Constant Q transform.\n",
    "\n",
    "In other words, a Constant Q transform will yield better results where low frequencies and logarithmic frequency mapping are concerned.\n",
    "\n",
    "The transform exhibits a reduction in frequency resolution with higher frequency bins, which is desirable for auditory applications. The transform mirrors the human auditory system, whereby at lower-frequencies spectral resolution is better, whereas temporal resolution improves at higher frequencies.\n",
    "\n",
    "CQT refers to a time-frequency representation where the frequency bins are geometrically spaced and the Q-factors (ratios of the center frequencies to bandwidths) of all bins are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5070a6ae",
   "metadata": {},
   "source": [
    "nnAudio\n",
    "nnAudio is an audio processing toolbox using PyTorch convolutional neural network as its backend. By doing so, spectrograms can be generated from audio on-the-fly during neural network training and the Fourier kernels (e.g. or CQT kernels) can be trained. Kapre and torch-stft have a similar concept in which they also use 1D convolution from Keras and PyTorch to do the waveforms to spectrogram conversions. Other GPU audio processing tools are torchaudio and tf.signal. But they are not using the neural network approach, and hence the Fourier basis can not be trained.\n",
    "\n",
    "From this Paper\n",
    "\n",
    "\"nnAudio, a new neural network-based audio processing framework with graphics processing unit (GPU) support that leverages 1D convolutional neural networks to perform time domain to frequency domain conversion. It allows on-the-fly spectrogram extraction due to its fast speed, without the need to store any spectrograms on the disk. Moreover, this approach also allows back-propagation on the waveforms-to-spectrograms transformation layer, and hence, the transformation process can be made trainable, further optimizing the waveform-to-spectrogram transformation for the specific task that the neural network is trained on. All spectrogram implementations scale as Big-O of linear time with respect to the input length. nnAudio, however, leverages the compute unified device architecture (CUDA) of 1D convolutional neural network from PyTorch, its short-time Fourier transform (STFT), Mel spectrogram, and constant-Q transform (CQT) implementations are an order of magnitude faster than other implementations using only the central processing unit (CPU).\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
