{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gwpy\n",
        "import gwpy\n",
        "from gwpy.timeseries import TimeSeries"
      ],
      "metadata": {
        "id": "gWYeAMc_2euc"
      },
      "id": "gWYeAMc_2euc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa"
      ],
      "metadata": {
        "id": "lPIku7jA2ial"
      },
      "id": "lPIku7jA2ial",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle"
      ],
      "metadata": {
        "id": "WYoBx-hI2kkn"
      },
      "id": "WYoBx-hI2kkn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "BKav9m6f2m6E"
      },
      "id": "BKav9m6f2m6E",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.upload()"
      ],
      "metadata": {
        "id": "0Ulqj9v62ogu"
      },
      "id": "0Ulqj9v62ogu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "t5tJchW32qP3"
      },
      "id": "t5tJchW32qP3",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "id": "TT8Tr-Xt2rlw"
      },
      "id": "TT8Tr-Xt2rlw",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "SpmVHlSy2tGa"
      },
      "id": "SpmVHlSy2tGa",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c g2net-gravitational-wave-detection"
      ],
      "metadata": {
        "id": "Q_7bTd7T2tJT"
      },
      "id": "Q_7bTd7T2tJT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir tfm_g2n"
      ],
      "metadata": {
        "id": "XqTHsd5c2tMG"
      },
      "id": "XqTHsd5c2tMG",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip g2net-gravitational-wave-detection.zip -d tfm_g2n"
      ],
      "metadata": {
        "id": "bV7aego62tOA"
      },
      "id": "bV7aego62tOA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "dvawHHcs2tQD"
      },
      "id": "dvawHHcs2tQD",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "91419897",
      "metadata": {
        "id": "91419897"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d476ec1a",
      "metadata": {
        "id": "d476ec1a"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.metrics import AUC"
      ],
      "metadata": {
        "id": "hhgHnc00YJj6"
      },
      "id": "hhgHnc00YJj6",
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9033f27e",
      "metadata": {
        "id": "9033f27e"
      },
      "source": [
        "## Setup variables"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = pd.read_csv(\"/content/tfm_g2n/training_labels.csv\")\n",
        "train_labels.head()"
      ],
      "metadata": {
        "id": "2YUlTXmyEJ7A"
      },
      "id": "2YUlTXmyEJ7A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84855949",
      "metadata": {
        "id": "84855949"
      },
      "outputs": [],
      "source": [
        "training_paths = glob(\"D:/Projects/G2Net-Gravitational-Wave-Detection/data/train/*/*/*/*\")\n",
        "print(\"The total number of files in the training set:\", len(training_paths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3520477",
      "metadata": {
        "id": "a3520477"
      },
      "outputs": [],
      "source": [
        "ids = [path.split(\"\\\\\")[-1].split(\".\")[0] for path in training_paths]\n",
        "paths_df = pd.DataFrame({\"path\":training_paths, \"id\": ids})\n",
        "train_data = pd.merge(left=training_labels, right=paths_df, on=\"id\")\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_data['id']\n",
        "y = train_data['target'].astype('int8').values"
      ],
      "metadata": {
        "id": "jsRb_qaiex1F"
      },
      "id": "jsRb_qaiex1F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_valid, y_train, y_valid = train_test_split(X, y, random_state = 42, stratify = y)"
      ],
      "metadata": {
        "id": "PHu4zrjue2hD"
      },
      "id": "PHu4zrjue2hD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign the test IDs\n",
        "sub = pd.read_csv(\"/content/tfm_g2n/sample_submission.csv\")\n",
        "x_test = sub[['id']]"
      ],
      "metadata": {
        "id": "9UuCEdpw8gPY"
      },
      "id": "9UuCEdpw8gPY",
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 250"
      ],
      "metadata": {
        "id": "EJs21Zawe6EZ"
      },
      "id": "EJs21Zawe6EZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (56,193,1)"
      ],
      "metadata": {
        "id": "HfJl5QQVe7o5"
      },
      "id": "HfJl5QQVe7o5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define the dataset object"
      ],
      "metadata": {
        "id": "LHuiAf9cfblK"
      },
      "id": "LHuiAf9cfblK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the data filepaths as tensor_slices\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train.apply(get_npy_filepath).values, y_train))\n",
        "# shuffle the dataset\n",
        "train_dataset = train_dataset.shuffle(len(x_train))\n",
        "# Apply the map method to tf_parse_function()\n",
        "train_dataset = train_dataset.map(preprocess_function_parse_tf, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "# set batcg size of the dataset\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "# Prefetch the data\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "LjX_2oZFe_2e"
      },
      "id": "LjX_2oZFe_2e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the data filepaths as tensor_slices\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((x_valid.apply(get_npy_filepath).values, y_valid))\n",
        "# Apply the map method to tf_parse_function()\n",
        "valid_dataset = valid_dataset.map(preprocess_function_parse_tf, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "# set batcg size of the dataset\n",
        "valid_dataset = valid_dataset.batch(batch_size)\n",
        "# Prefetch the data\n",
        "valid_dataset = valid_dataset.prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "w1W0CJIffDex"
      },
      "id": "w1W0CJIffDex",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the data filepaths as tensor_slices\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test[\"id\"].apply(get_npy_filepath, is_train=False).values))\n",
        "# Apply the map method to tf_parse_function()\n",
        "test_dataset = test_dataset.map(preprocess_function_parse_tf, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "# set batcg size of the dataset\n",
        "test_dataset = test_dataset.batch(batch_size)\n",
        "# Prefetch the data\n",
        "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "NHkcORUWf-SJ"
      },
      "id": "NHkcORUWf-SJ",
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CQT\n",
        "transform = CQT1992v2(sr=2048,        # sample rate\n",
        "                fmin=20,        # min freq\n",
        "                fmax=500,      # max freq\n",
        "                hop_length=64,  # hop length\n",
        "                verbose=False)"
      ],
      "metadata": {
        "id": "ukAEk_KEfHqq"
      },
      "id": "ukAEk_KEfHqq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's define some signal parameters\n",
        "sample_rate = 2048 # data is provided at 2048 Hz\n",
        "signal_length = 2 # each signal lasts 2 s\n",
        "fmin, fmax = 20, 1024 # filter above 20 Hz, and max 1024 Hz (Nyquist freq = sample_rate/2)\n",
        "hop_length = 64 # hop length parameter for the stft\n",
        "\n",
        "# model compile params\n",
        "batch_size = 250 # size in which data is processed and trained at-once in model\n",
        "epochs = 3 # number of epochs (keep low as dataset is quite large 3~5 is enough as observed)"
      ],
      "metadata": {
        "id": "3HB4kc-18RWY"
      },
      "id": "3HB4kc-18RWY",
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions"
      ],
      "metadata": {
        "id": "szeWpk3lel8u"
      },
      "id": "szeWpk3lel8u"
    },
    {
      "cell_type": "code",
      "source": [
        "# function to load the file, preprocess, return the respective Constant Q-transform\n",
        "# the Cqt function\n",
        "# preprocess function\n",
        "def parse_function(id_path):\n",
        "    # load the npy file\n",
        "    signals = np.load(id_path.numpy())\n",
        "    \n",
        "    # loop through each signal\n",
        "    for i in range(signals.shape[0]):\n",
        "        # normalize the signal data\n",
        "        signals[i] /= np.max(signals[i])\n",
        "    \n",
        "    # stack the arrays into a single vector\n",
        "    signals = np.hstack(signals)\n",
        "    \n",
        "    # convert the signals to torch.tensor to pass to CQT\n",
        "    signals = torch.from_numpy(signals).float()\n",
        "    \n",
        "    # get the CQT\n",
        "    image = cq_transform(signals)\n",
        "    \n",
        "    # conver the image from torch.tensor to array\n",
        "    image = np.array(image)\n",
        "    \n",
        "    # transpose the image to get right orientation\n",
        "    image = np.transpose(image,(1,2,0))\n",
        "    \n",
        "    # conver the image to tf.tensor and return\n",
        "    return tf.convert_to_tensor(image)"
      ],
      "metadata": {
        "id": "fGMEBz4wggCm"
      },
      "id": "fGMEBz4wggCm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the tf_function which is called in the data pipeline. This runs as TF function\n",
        "def tf_parse_function(id_path, y=None):\n",
        "    # pass the id_path to the py_function parse_function\n",
        "    [x] = tf.py_function(func=parse_function, inp=[id_path], Tout=[tf.float32])\n",
        "    \n",
        "#     x.set_shape(signal_shape) # signal_shape\n",
        "    x = tf.ensure_shape(x, input_shape)\n",
        "    \n",
        "    # if train/valid then return x, y; for test only return x\n",
        "    if y is None:\n",
        "        return x\n",
        "    else:\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "WYXw4YSFgnzO"
      },
      "id": "WYXw4YSFgnzO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_npy_filepath(id_, is_train=True):\n",
        "    path = ''\n",
        "    if is_train:\n",
        "        return f'/content/tfm_g2n/train/{id_[0]}/{id_[1]}/{id_[2]}/{id_}.npy'\n",
        "    else:\n",
        "        return f'/content/tfm_g2n/test/{id_[0]}/{id_[1]}/{id_[2]}/{id_}.npy'"
      ],
      "metadata": {
        "id": "JL3zByv1erYd"
      },
      "id": "JL3zByv1erYd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to save kaggle submissions for test prediction probabilities\n",
        "def get_kaggle_format(prediction_probs, model='base'):\n",
        "    # load the sample submission file\n",
        "    sub = pd.read_csv(\"/content/tfm_g2n/sample_submission.csv\")\n",
        "    sub['target'] = prediction_probs\n",
        "    \n",
        "    # Output filename for kaggle submission\n",
        "    filename = f\"kaggle_sub_{model}.csv\"\n",
        "    \n",
        "    # Save the DataFrame to a file\n",
        "    sub.to_csv(filename, index=False)\n",
        "    print(f'File name: {filename}')"
      ],
      "metadata": {
        "id": "st4xV9OqhEWv"
      },
      "id": "st4xV9OqhEWv",
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "be8d37e8",
      "metadata": {
        "id": "be8d37e8"
      },
      "source": [
        "# Modelo de https://github.com/PraveenThakkannavar/G2Net-Gravitational-Wave-Detection/blob/main/SIMPLE_CNN.ipynb y delhttps://github.com/SiddharthPatel45/gravitational-wave-detection/blob/main/code/gw-detection-modelling.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "id": "b0d59949",
      "metadata": {
        "id": "b0d59949"
      },
      "outputs": [],
      "source": [
        "# Instantiate the Sequential model\n",
        "model_cnn = Sequential(name='CNN_model')\n",
        "\n",
        "# Add the first Convoluted2D layer w/ input_shape & MaxPooling2D layer followed by that\n",
        "model_cnn.add(Conv2D(filters=16,\n",
        "                     kernel_size=3,\n",
        "                     input_shape=input_shape,\n",
        "                     activation='relu',\n",
        "                     name='Conv_01'))\n",
        "model_cnn.add(MaxPooling2D(pool_size=2, name='Pool_01'))\n",
        "\n",
        "# Second pair of Conv1D and MaxPooling1D layers\n",
        "model_cnn.add(Conv2D(filters=32,\n",
        "                     kernel_size=3,\n",
        "                     input_shape=input_shape,\n",
        "                     activation='relu',\n",
        "                     name='Conv_02'))\n",
        "model_cnn.add(MaxPooling2D(pool_size=2, name='Pool_02'))\n",
        "\n",
        "# Third pair of Conv1D and MaxPooling1D layers\n",
        "model_cnn.add(Conv2D(filters=64,\n",
        "                     kernel_size=3,\n",
        "                     input_shape=input_shape,\n",
        "                     activation='relu',\n",
        "                     name='Conv_03'))\n",
        "model_cnn.add(MaxPooling2D(pool_size=2, name='Pool_03'))\n",
        "\n",
        "# Add the Flatten layer\n",
        "model_cnn.add(Flatten(name='Flatten'))\n",
        "\n",
        "# Add the Dense layers\n",
        "model_cnn.add(Dense(units=512,\n",
        "                activation='relu',\n",
        "                name='Dense_01'))\n",
        "model_cnn.add(Dense(units=64,\n",
        "                activation='relu',\n",
        "                name='Dense_02'))\n",
        "\n",
        "# Add the final Output layer\n",
        "model_cnn.add(Dense(1, activation='sigmoid', name='Output'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "6475ea77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "6475ea77",
        "outputId": "0c471c1d-7eee-4691-aa64-c00fb7d3926d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"CNN_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Conv_01 (Conv2D)            (None, 54, 191, 16)       160       \n",
            "                                                                 \n",
            " Pool_01 (MaxPooling2D)      (None, 27, 95, 16)        0         \n",
            "                                                                 \n",
            " Conv_02 (Conv2D)            (None, 25, 93, 32)        4640      \n",
            "                                                                 \n",
            " Pool_02 (MaxPooling2D)      (None, 12, 46, 32)        0         \n",
            "                                                                 \n",
            " Conv_03 (Conv2D)            (None, 10, 44, 64)        18496     \n",
            "                                                                 \n",
            " Pool_03 (MaxPooling2D)      (None, 5, 22, 64)         0         \n",
            "                                                                 \n",
            " Flatten (Flatten)           (None, 7040)              0         \n",
            "                                                                 \n",
            " Dense_01 (Dense)            (None, 512)               3604992   \n",
            "                                                                 \n",
            " Dense_02 (Dense)            (None, 64)                32832     \n",
            "                                                                 \n",
            " Output (Dense)              (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,661,185\n",
            "Trainable params: 3,661,185\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_cnn.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "d3de0418",
      "metadata": {
        "id": "d3de0418"
      },
      "outputs": [],
      "source": [
        "model_cnn.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=[[AUC(), 'accuracy']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "id": "6e4a45b2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "6e4a45b2",
        "outputId": "0a39e821-3554-446a-a9e4-a99fbe1e6caa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1680/1680 [==============================] - 1853s 1s/step - loss: 0.5256 - auc_1: 0.7982 - accuracy: 0.7223 - val_loss: 0.4842 - val_auc_1: 0.8296 - val_accuracy: 0.7563\n",
            "Epoch 2/3\n",
            "1680/1680 [==============================] - 1840s 1s/step - loss: 0.4866 - auc_1: 0.8261 - accuracy: 0.7538 - val_loss: 0.4761 - val_auc_1: 0.8336 - val_accuracy: 0.7620\n",
            "Epoch 3/3\n",
            "1680/1680 [==============================] - 1823s 1s/step - loss: 0.4795 - auc_1: 0.8302 - accuracy: 0.7579 - val_loss: 0.4879 - val_auc_1: 0.8351 - val_accuracy: 0.7484\n"
          ]
        }
      ],
      "source": [
        "# Fit the data\n",
        "history_cnn = model_cnn.fit(x=train_dataset,\n",
        "                            epochs=3,\n",
        "                            validation_data=valid_dataset,\n",
        "                            batch_size=250,\n",
        "                            verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# re-train the model on remaining validation data\n",
        "model_cnn.fit(x=valid_dataset, epochs=epochs, batch_size=batch_size, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "2hlQ2d9uYrRJ",
        "outputId": "81738791-3de2-4bc5-9744-14f410000a8d"
      },
      "id": "2hlQ2d9uYrRJ",
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "560/560 [==============================] - 472s 843ms/step - loss: 0.4740 - auc_1: 0.8330 - accuracy: 0.7622\n",
            "Epoch 2/3\n",
            "560/560 [==============================] - 473s 844ms/step - loss: 0.4722 - auc_1: 0.8342 - accuracy: 0.7633\n",
            "Epoch 3/3\n",
            "560/560 [==============================] - 469s 837ms/step - loss: 0.4709 - auc_1: 0.8351 - accuracy: 0.7644\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f669692b950>"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predict the test dataset using CNN\n",
        "preds_cnn = model_cnn.predict(test_dataset)"
      ],
      "metadata": {
        "id": "9BwIEhJtg4_y"
      },
      "id": "9BwIEhJtg4_y",
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save the kaggle submission file\n",
        "get_kaggle_format(preds_cnn, model='cnn')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "yFGizozEg7kj",
        "outputId": "fe480585-8853-4612-c166-e9a80e616828"
      },
      "id": "yFGizozEg7kj",
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File name: kaggle_sub_cnn.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model\n",
        "model_cnn.save('/content/tfm_g2n/model_CNN.h5')"
      ],
      "metadata": {
        "id": "bp_U66NEJzd5"
      },
      "id": "bp_U66NEJzd5",
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the predictions into a dataframe\n",
        "df_preds_cnn = pd.read_csv('/content/tfm_g2n/kaggle_sub_cnn.csv')\n",
        "df_preds_cnn.head()"
      ],
      "metadata": {
        "id": "TtycwSvSheOv"
      },
      "id": "TtycwSvSheOv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_preds_cnn.shape"
      ],
      "metadata": {
        "id": "sD7UcMSmhhxo"
      },
      "id": "sD7UcMSmhhxo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_preds_cnn[(df_preds_cnn['target'] >= 0.9) | (df_preds_cnn['target'] <= 0.1)]['target'].count()"
      ],
      "metadata": {
        "id": "5TGRTBQahh2Q"
      },
      "id": "5TGRTBQahh2Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_preds_cnn[(df_preds_cnn['target'] >= 0.8) | (df_preds_cnn['target'] <= 0.2)]['target'].count("
      ],
      "metadata": {
        "id": "W3Vt0Dq9hpe_"
      },
      "id": "W3Vt0Dq9hpe_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TvKbGnubhpid"
      },
      "id": "TvKbGnubhpid",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1xx4R4Ghhplq"
      },
      "id": "1xx4R4Ghhplq",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "name": "03_Modelling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}