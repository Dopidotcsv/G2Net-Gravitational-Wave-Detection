{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b221ae6",
   "metadata": {},
   "source": [
    "https://github.com/JonasHeinzmann-AI/G2Net-Gravitational-Wave-Detection/blob/main/g2net-eda-and-modeling.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faade12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install efficientnet_pytorch -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e277932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data as torch_data\n",
    "from sklearn import model_selection as sk_model_selection\n",
    "from torch.nn import functional as torch_functional\n",
    "from torch.autograd import Variable\n",
    "import efficientnet_pytorch\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f9e7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6806d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataRetriever(torch_data.Dataset):\n",
    "    def __init__(self, paths, targets):\n",
    "        self.paths = paths\n",
    "        self.targets = targets\n",
    "        \n",
    "        self.q_transform = CQT1992v2(\n",
    "            sr=2048, fmin=20, fmax=1024, hop_length=32\n",
    "        )\n",
    "          \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __get_qtransform(self, x):\n",
    "        image = []\n",
    "        for i in range(3):\n",
    "            waves = x[i] / np.max(x[i])\n",
    "            waves = torch.from_numpy(waves).float()\n",
    "            channel = self.q_transform(waves).squeeze().numpy()\n",
    "            image.append(channel)\n",
    "            \n",
    "        return torch.tensor(image).float()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        file_path = convert_image_id_2_path(self.paths[index])\n",
    "        x = np.load(file_path)\n",
    "        image = self.__get_qtransform(x)\n",
    "        \n",
    "        y = torch.tensor(self.targets[index], dtype=torch.float)\n",
    "            \n",
    "        return {\"X\": image, \"y\": y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949183e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = efficientnet_pytorch.EfficientNet.from_pretrained(\"efficientnet-b7\")\n",
    "        n_features = self.net._fc.in_features\n",
    "        self.net._fc = nn.Linear(in_features=n_features, out_features=1, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b70a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossMeter:\n",
    "    def __init__(self):\n",
    "        self.avg = 0\n",
    "        self.n = 0\n",
    "\n",
    "    def update(self, val):\n",
    "        self.n += 1\n",
    "        # incremental update\n",
    "        self.avg = val / self.n + (self.n - 1) / self.n * self.avg\n",
    "\n",
    "        \n",
    "class AccMeter:\n",
    "    def __init__(self):\n",
    "        self.avg = 0\n",
    "        self.n = 0\n",
    "        \n",
    "    def update(self, y_true, y_pred):\n",
    "        y_true = y_true.cpu().numpy().astype(int)\n",
    "        y_pred = y_pred.cpu().numpy() >= 0\n",
    "        last_n = self.n\n",
    "        self.n += len(y_true)\n",
    "        true_count = np.sum(y_true == y_pred)\n",
    "        # incremental update\n",
    "        self.avg = true_count / self.n + last_n / self.n * self.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95e1794",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model, \n",
    "        device, \n",
    "        optimizer, \n",
    "        criterion, \n",
    "        loss_meter, \n",
    "        score_meter\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.loss_meter = loss_meter\n",
    "        self.score_meter = score_meter\n",
    "        \n",
    "        self.best_valid_score = -np.inf\n",
    "        self.n_patience = 0\n",
    "        \n",
    "        self.messages = {\n",
    "            \"epoch\": \"[Epoch {}: {}] loss: {:.5f}, score: {:.5f}, time: {} s\",\n",
    "            \"checkpoint\": \"The score improved from {:.5f} to {:.5f}. Save model to '{}'\",\n",
    "            \"patience\": \"\\nValid score didn't improve last {} epochs.\"\n",
    "        }\n",
    "    \n",
    "    def fit(self, epochs, train_loader, valid_loader, save_path, patience):        \n",
    "        for n_epoch in range(1, epochs + 1):\n",
    "            self.info_message(\"EPOCH: {}\", n_epoch)\n",
    "            \n",
    "            train_loss, train_score, train_time = self.train_epoch(train_loader)\n",
    "            valid_loss, valid_score, valid_time = self.valid_epoch(valid_loader)\n",
    "            \n",
    "            self.info_message(\n",
    "                self.messages[\"epoch\"], \"Train\", n_epoch, train_loss, train_score, train_time\n",
    "            )\n",
    "            \n",
    "            self.info_message(\n",
    "                self.messages[\"epoch\"], \"Valid\", n_epoch, valid_loss, valid_score, valid_time\n",
    "            )\n",
    "\n",
    "            if True:\n",
    "#             if self.best_valid_score < valid_score:\n",
    "                self.info_message(\n",
    "                    self.messages[\"checkpoint\"], self.best_valid_score, valid_score, save_path\n",
    "                )\n",
    "                self.best_valid_score = valid_score\n",
    "                self.save_model(n_epoch, save_path)\n",
    "                self.n_patience = 0\n",
    "            else:\n",
    "                self.n_patience += 1\n",
    "            \n",
    "            if self.n_patience >= patience:\n",
    "                self.info_message(self.messages[\"patience\"], patience)\n",
    "                break\n",
    "            \n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        t = time.time()\n",
    "        train_loss = self.loss_meter()\n",
    "        train_score = self.score_meter()\n",
    "        \n",
    "        for step, batch in enumerate(train_loader, 1):\n",
    "            X = batch[\"X\"].to(self.device)\n",
    "            targets = batch[\"y\"].to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(X).squeeze(1)\n",
    "            \n",
    "            loss = self.criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "\n",
    "            train_loss.update(loss.detach().item())\n",
    "            train_score.update(targets, outputs.detach())\n",
    "\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            _loss, _score = train_loss.avg, train_score.avg\n",
    "            message = 'Train Step {}/{}, train_loss: {:.5f}, train_score: {:.5f}'\n",
    "            self.info_message(message, step, len(train_loader), _loss, _score, end=\"\\r\")\n",
    "        \n",
    "        return train_loss.avg, train_score.avg, int(time.time() - t)\n",
    "    \n",
    "    def valid_epoch(self, valid_loader):\n",
    "        self.model.eval()\n",
    "        t = time.time()\n",
    "        valid_loss = self.loss_meter()\n",
    "        valid_score = self.score_meter()\n",
    "\n",
    "        for step, batch in enumerate(valid_loader, 1):\n",
    "            with torch.no_grad():\n",
    "                X = batch[\"X\"].to(self.device)\n",
    "                targets = batch[\"y\"].to(self.device)\n",
    "\n",
    "                outputs = self.model(X).squeeze(1)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "\n",
    "                valid_loss.update(loss.detach().item())\n",
    "                valid_score.update(targets, outputs)\n",
    "                \n",
    "            _loss, _score = valid_loss.avg, valid_score.avg\n",
    "            message = 'Valid Step {}/{}, valid_loss: {:.5f}, valid_score: {:.5f}'\n",
    "            self.info_message(message, step, len(valid_loader), _loss, _score, end=\"\\r\")\n",
    "        \n",
    "        return valid_loss.avg, valid_score.avg, int(time.time() - t)\n",
    "    \n",
    "    def save_model(self, n_epoch, save_path):\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": self.model.state_dict(),\n",
    "                \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "                \"best_valid_score\": self.best_valid_score,\n",
    "                \"n_epoch\": n_epoch,\n",
    "            },\n",
    "            save_path,\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def info_message(message, *args, end=\"\\n\"):\n",
    "        print(message.format(*args), end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6981bab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=2, random_state=42, shuffle=True)\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(train_df, train_df[\"target\"])):\n",
    "    train_X = train_df.iloc[train_index]\n",
    "    valid_X = train_df.iloc[valid_index][:20000] # Reduce calculation time\n",
    "    print(train_X.shape, valid_X.shape)\n",
    "\n",
    "    train_data_retriever = DataRetriever(\n",
    "        train_X[\"id\"].values, \n",
    "        train_X[\"target\"].values, \n",
    "    )\n",
    "\n",
    "    valid_data_retriever = DataRetriever(\n",
    "        valid_X[\"id\"].values, \n",
    "        valid_X[\"target\"].values,\n",
    "    )\n",
    "\n",
    "    train_loader = torch_data.DataLoader(\n",
    "        train_data_retriever,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "    )\n",
    "\n",
    "    valid_loader = torch_data.DataLoader(\n",
    "        valid_data_retriever, \n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "    )\n",
    "\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = torch_functional.binary_cross_entropy_with_logits\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model, \n",
    "        device, \n",
    "        optimizer, \n",
    "        criterion, \n",
    "        LossMeter, \n",
    "        AccMeter\n",
    "    )\n",
    "\n",
    "    history = trainer.fit(\n",
    "        1, \n",
    "        train_loader, \n",
    "        valid_loader, \n",
    "        f\"best-model-{fold}.pth\", \n",
    "        100,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d36ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for i in range(2):\n",
    "    model = Model()\n",
    "    model.to(device)\n",
    "    \n",
    "    checkpoint = torch.load(f\"best-model-{i}.pth\")\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "    \n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73594312",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataRetriever(torch_data.Dataset):\n",
    "    def __init__(self, paths):\n",
    "        self.paths = paths\n",
    "\n",
    "        self.q_transform = CQT1992v2(\n",
    "            sr=2048, fmin=20, fmax=1024, hop_length=32\n",
    "        )\n",
    "          \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __get_qtransform(self, x):\n",
    "        image = []\n",
    "        for i in range(3):\n",
    "            waves = x[i] / np.max(x[i])\n",
    "            waves = torch.from_numpy(waves).float()\n",
    "            channel = self.q_transform(waves).squeeze().numpy()\n",
    "            image.append(channel)\n",
    "            \n",
    "        return torch.tensor(image).float()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        file_path = convert_image_id_2_path(self.paths[index], is_train=False)\n",
    "        x = np.load(file_path)\n",
    "        image = self.__get_qtransform(x)\n",
    "            \n",
    "        return {\"X\": image, \"id\": self.paths[index]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea33518",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_retriever = DataRetriever(\n",
    "    submission[\"id\"].values, \n",
    ")\n",
    "\n",
    "test_loader = torch_data.DataLoader(\n",
    "    test_data_retriever,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef11692",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "ids = []\n",
    "\n",
    "for e, batch in enumerate(test_loader):\n",
    "    print(f\"{e}/{len(test_loader)}\", end=\"\\r\")\n",
    "    with torch.no_grad():\n",
    "        tmp_pred = np.zeros((batch[\"X\"].shape[0], ))\n",
    "        for model in models:\n",
    "            tmp_res = torch.sigmoid(model(batch[\"X\"].to(device))).cpu().numpy().squeeze()\n",
    "            tmp_pred += tmp_res / 2\n",
    "        y_pred.extend(tmp_pred)\n",
    "        ids.extend(batch[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40fb682",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"id\": ids, \"target\": y_pred})\n",
    "submission.to_csv(\"model_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd545eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9c10ad",
   "metadata": {},
   "source": [
    "https://github.com/JonasHeinzmann-AI/G2Net-Gravitational-Wave-Detection/blob/main/g2net-pytorch-ext.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd364b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is simply pass data that Q-transformed with the size (x,y,3) to an EfficientNet, \n",
    "#the fully connected layer near the end of EfficientNet replaced by another fully connected \n",
    "#layer that serve our purpose.\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = efficientnet_pytorch.EfficientNet.from_pretrained(\"efficientnet-b7\")\n",
    "        n_features = self.net._fc.in_features\n",
    "        self.net._fc = nn.Linear(in_features=n_features, out_features=1, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548201c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossMeter:\n",
    "    def __init__(self):\n",
    "        self.avg = 0\n",
    "        self.n = 0\n",
    "\n",
    "    def update(self, val):\n",
    "        self.n += 1\n",
    "        # incremental update\n",
    "        self.avg = val / self.n + (self.n - 1) / self.n * self.avg\n",
    "\n",
    "        \n",
    "class AccMeter:\n",
    "    def __init__(self):\n",
    "        self.avg = 0\n",
    "        self.n = 0\n",
    "        \n",
    "    def update(self, y_true, y_pred):\n",
    "        y_true = y_true.cpu().numpy().astype(int)\n",
    "        y_pred = y_pred.cpu().numpy() >= 0\n",
    "        last_n = self.n\n",
    "        self.n += len(y_true)\n",
    "        true_count = np.sum(y_true == y_pred)\n",
    "        # incremental update\n",
    "        self.avg = true_count / self.n + last_n / self.n * self.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04475c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.synchronize\n",
    "\n",
    "    model = Model()\n",
    "    model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.8)\n",
    "    criterion = torch_functional.binary_cross_entropy_with_logits\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        device,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        LossMeter,\n",
    "        AccMeter\n",
    "    )\n",
    "\n",
    "    history = trainer.fit(\n",
    "        2,\n",
    "        train_loader,\n",
    "        valid_loader,\n",
    "        \"best-model.pth\",\n",
    "        100,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb28cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c0f8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load(\"best-model.pth\")\n",
    "\n",
    "# model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "# model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc2e389",
   "metadata": {},
   "source": [
    "https://github.com/SiddharthPatel45/gravitational-wave-detection/blob/main/code/gw-detection-modelling.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d66d0c9",
   "metadata": {},
   "source": [
    "baseline score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191995c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target distribution in the training data\n",
    "train['target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c3dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define some signal parameters\n",
    "sample_rate = 2048 # data is provided at 2048 Hz\n",
    "signal_length = 2 # each signal lasts 2 s\n",
    "fmin, fmax = 20, 1024 # filter above 20 Hz, and max 1024 Hz (Nyquist freq = sample_rate/2)\n",
    "hop_length = 64 # hop length parameter for the stft\n",
    "\n",
    "# model compile params\n",
    "batch_size = 250 # size in which data is processed and trained at-once in model\n",
    "epochs = 3 # number of epochs (keep low as dataset is quite large 3~5 is enough as observed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1b1bfd",
   "metadata": {},
   "source": [
    "tf data pipeline ( preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f29f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return the npy file corresponding to the id\n",
    "def get_npy_filepath(id_, is_train=True):\n",
    "    path = ''\n",
    "    if is_train:\n",
    "        return f'../input/g2net-gravitational-wave-detection/train/{id_[0]}/{id_[1]}/{id_[2]}/{id_}.npy'\n",
    "    else:\n",
    "        return f'../input/g2net-gravitational-wave-detection/test/{id_[0]}/{id_[1]}/{id_[2]}/{id_}.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66c2222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Constant Q-Transform\n",
    "cq_transform = CQT1992v2(sr=sample_rate, fmin=fmin, fmax=fmax, hop_length=hop_length)\n",
    "\n",
    "# check if GPU enabled, then run the transform on GPU for faster execution\n",
    "# if tf.test.is_gpu_available():\n",
    "#     cq_transform = cq_transform.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8785d2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the file, preprocess, return the respective Constant Q-transform\n",
    "def parse_function(id_path):\n",
    "    # load the npy file\n",
    "    signals = np.load(id_path.numpy())\n",
    "    \n",
    "    # loop through each signal\n",
    "    for i in range(signals.shape[0]):\n",
    "        # normalize the signal data\n",
    "        signals[i] /= np.max(signals[i])\n",
    "    \n",
    "    # stack the arrays into a single vector\n",
    "    signals = np.hstack(signals)\n",
    "    \n",
    "    # convert the signals to torch.tensor to pass to CQT\n",
    "    signals = torch.from_numpy(signals).float()\n",
    "    \n",
    "    # get the CQT\n",
    "    image = cq_transform(signals)\n",
    "    \n",
    "    # conver the image from torch.tensor to array\n",
    "    image = np.array(image)\n",
    "    \n",
    "    # transpose the image to get right orientation\n",
    "    image = np.transpose(image,(1,2,0))\n",
    "    \n",
    "    # conver the image to tf.tensor and return\n",
    "    return tf.convert_to_tensor(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe435ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a sample\n",
    "image = parse_function(tf.convert_to_tensor(get_npy_filepath(train['id'].values[0])))\n",
    "print(image.shape)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd314f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# From the Constant Q-Transform that we got, get the shape\n",
    "input_shape = (69, 193, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ed6e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature ids and target\n",
    "X = train[['id']]\n",
    "y = train['target'].astype('int8').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a470c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training IDs into training & validation datasets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=42, stratify=y)\n",
    "\n",
    "# Assign the test IDs\n",
    "X_test = sample_sub[['id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa78de51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the tf_function which is called in the data pipeline. This runs as TF function\n",
    "def tf_parse_function(id_path, y=None):\n",
    "    # pass the id_path to the py_function parse_function\n",
    "    [x] = tf.py_function(func=parse_function, inp=[id_path], Tout=[tf.float32])\n",
    "    \n",
    "#     x.set_shape(signal_shape) # signal_shape\n",
    "    x = tf.ensure_shape(x, input_shape)\n",
    "    \n",
    "    # if train/valid then return x, y; for test only return x\n",
    "    if y is None:\n",
    "        return x\n",
    "    else:\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2d8c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "# Get the data filepaths as tensor_slices\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train['id'].apply(get_npy_filepath).values, y_train))\n",
    "\n",
    "# shuffle the dataset\n",
    "train_dataset = train_dataset.shuffle(len(X_train))\n",
    "\n",
    "# apply the map method to tf_parse_function()\n",
    "train_dataset = train_dataset.map(tf_parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# set batch size of the dataset\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "# prefetch the data\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9646cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid dataset\n",
    "# Get the data filepaths as tensor_slices\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid['id'].apply(get_npy_filepath).values, y_valid))\n",
    "\n",
    "# apply the map method to tf_parse_function()\n",
    "valid_dataset = valid_dataset.map(tf_parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# set batch size of the dataset\n",
    "valid_dataset = valid_dataset.batch(batch_size)\n",
    "\n",
    "# prefetch the data\n",
    "valid_dataset = valid_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c587978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset\n",
    "# Get the data filepaths as tensor_slices\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test['id'].apply(get_npy_filepath, is_train=False).values))\n",
    "\n",
    "# apply the map method to tf_parse_function()\n",
    "test_dataset = test_dataset.map(tf_parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# set batch size of the dataset\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "# prefetch the data\n",
    "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6895d6a2",
   "metadata": {},
   "source": [
    "modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5206136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the signal input shape\n",
    "for x, _ in train_dataset.take(1):\n",
    "    input_shape = x.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df00a580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to print the learning curves for the models\n",
    "# adapted from https://www.tensorflow.org/tutorials/images/transfer_learning#learning_curves\n",
    "def learning_curves(history, model='cnn'):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    if model == 'efn':\n",
    "        auc = history.history['auc_2']\n",
    "        val_auc = history.history['val_auc_2']\n",
    "    else:\n",
    "        auc = history.history['auc']\n",
    "        val_auc = history.history['val_auc']\n",
    "    \n",
    "    plt.figure(figsize=(12, 12))    \n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(auc, label='Training AUC')\n",
    "    plt.plot(val_auc, label='Validation AUC')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.ylim([0,1.0])\n",
    "    plt.title('Training and Validation AUC')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([min(plt.ylim()),1])\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4283385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Sequential model\n",
    "model_cnn = Sequential(name='CNN_model')\n",
    "\n",
    "# Add the first Convoluted2D layer w/ input_shape & MaxPooling2D layer followed by that\n",
    "model_cnn.add(Conv2D(filters=16,\n",
    "                     kernel_size=3,\n",
    "                     input_shape=input_shape,\n",
    "                     activation='relu',\n",
    "                     name='Conv_01'))\n",
    "model_cnn.add(MaxPooling2D(pool_size=2, name='Pool_01'))\n",
    "\n",
    "# Second pair of Conv1D and MaxPooling1D layers\n",
    "model_cnn.add(Conv2D(filters=32,\n",
    "                     kernel_size=3,\n",
    "                     input_shape=input_shape,\n",
    "                     activation='relu',\n",
    "                     name='Conv_02'))\n",
    "model_cnn.add(MaxPooling2D(pool_size=2, name='Pool_02'))\n",
    "\n",
    "# Third pair of Conv1D and MaxPooling1D layers\n",
    "model_cnn.add(Conv2D(filters=64,\n",
    "                     kernel_size=3,\n",
    "                     input_shape=input_shape,\n",
    "                     activation='relu',\n",
    "                     name='Conv_03'))\n",
    "model_cnn.add(MaxPooling2D(pool_size=2, name='Pool_03'))\n",
    "\n",
    "# Add the Flatten layer\n",
    "model_cnn.add(Flatten(name='Flatten'))\n",
    "\n",
    "# Add the Dense layers\n",
    "model_cnn.add(Dense(units=512,\n",
    "                activation='relu',\n",
    "                name='Dense_01'))\n",
    "model_cnn.add(Dense(units=64,\n",
    "                activation='relu',\n",
    "                name='Dense_02'))\n",
    "\n",
    "# Add the final Output layer\n",
    "model_cnn.add(Dense(1, activation='sigmoid', name='Output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fbf0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the CNN model architecture\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9728cd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model with following parameters\n",
    "# Optimizer: Adam (learning_rate=0.0001)\n",
    "# loss: binary_crossentropy\n",
    "# metrics: accuracy/AUC\n",
    "model_cnn.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=[[AUC(), 'accuracy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29067b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data\n",
    "history_cnn = model_cnn.fit(x=train_dataset,\n",
    "                            epochs=epochs,\n",
    "                            validation_data=valid_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6f20c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_cnn.save('./model_CNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74240501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
