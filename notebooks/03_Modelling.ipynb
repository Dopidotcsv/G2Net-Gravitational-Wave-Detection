{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91419897",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d476ec1a",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8778f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********** FOR GOOGLE DRIVE AND COLAB *****************\n",
    "\n",
    "import os \n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "\n",
    "!python -m pip install gwpy\n",
    "!pip install --upgrade --force-reinstall --no-deps gwpy\n",
    "!pip install astropy\n",
    "!pip install nnAudio\n",
    "!pip install colorama\n",
    "\n",
    "!pip install --upgrade --force-reinstall --no-deps matplotlib\n",
    "\n",
    "!pip install --force-reinstall --no-deps matplotlib==3.2.2\n",
    "# For running in Colab I have to have a previous version of matplotlib\n",
    "# This for Gihut Issue > https://github.com/gwpy/gwpy/issues/1398\n",
    "# More details are in my note in previous cell\n",
    "\n",
    "!pip install gwosc\n",
    "!pip install dqsegdb2\n",
    "!pip install ligotimegps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fee3102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from gwpy.timeseries import TimeSeries\n",
    "from gwpy.plot import Plot\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from colorama import Fore, Back, Style\n",
    "plt.style.use('ggplot')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPool1D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "import torch\n",
    "from nnAudio.Spectrogram import CQT1992v2\n",
    "\n",
    "\n",
    "from src.model.model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c082704",
   "metadata": {},
   "source": [
    "## Setup variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80768314",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/data_path.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edea4bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the contents of one file\n",
    "\n",
    "\n",
    "\n",
    "# LOCAL ROOT DIRECTOR\n",
    "root_dir = \"C:/Users/e107338/PycharmProjects/G2Net-Gravitational-Wave-Detection/data\"\n",
    "file = root_dir + 'train/0/0/0/000a5b6e5c.npy'\n",
    "data = np.load(file)\n",
    "print(data.shape)\n",
    "print(data)\n",
    "# print(data[0, :].shape)\n",
    "# print(data[1, :].shape)\n",
    "# print(data[2, :].shape)\n",
    "print(\"data[0, :] is \", data[0, :])\n",
    "# data_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28de64a8",
   "metadata": {},
   "source": [
    "Load the .npy files from all the nested folder-structure and get the ids from file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcef259",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"D:/Projects/G2Net-Gravitational-Wave-Detection/data\"\n",
    "train_labels = pd.read_csv(root_dir + \"/training_labels.csv\")\n",
    "print('Dataset has ' + \"{} Observations\".format(train_labels.shape[0]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5d0042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a Training dataframe for all the available .npy files \n",
    "\n",
    "# Get all the file file path from all 4-labels of nested folder structure\n",
    "files_paths = glob(root_dir + '/train/*/*/*/*')\n",
    "''' The glob module finds all the pathnames matching a specified pattern according to the rules \n",
    "used by the Unix shell, although results are returned in arbitrary order. \n",
    "No tilde expansion is done, but *, ?, and character ranges expressed with [] will be correctly matched. \n",
    "We can use glob to search for a specific file pattern, or perhaps more usefully, search for files where the \n",
    "filename matches a certain pattern by using wildcard characters.\n",
    "\n",
    "'''\n",
    "\n",
    "# get the list of ids from the .npy files\n",
    "ids_from_npy_files = [path.split(\"/\")[-1].split(\".\")[0] for path in files_paths]\n",
    "# [-1] means the last element in a sequence,\n",
    "\n",
    "# get a dataframe with paths and ids of those .npy files\n",
    "df_path_id = pd.DataFrame({'path': files_paths, 'id':ids_from_npy_files})\n",
    "df_path_id.head()\n",
    "\n",
    "# merging that above df with the target\n",
    "df_train = pd.merge(left=train_labels, right=df_path_id, on='id')\n",
    "display(df_train.head())\n",
    "\n",
    "# verifying the shape of the merged df has 5,60,000 rows and 3 columns\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9666c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(root_dir +  'sample_submission.csv')\n",
    "print(len(train_labels)) # 5,60,000\n",
    "print(len(sample_submission)) # 2,260,000\n",
    "train_ids = train_labels['id'].values\n",
    "# train_ids # ['00000e74ad', '00001f4945', '0000661522' ... ]\n",
    "y = train_labels['target'].values\n",
    "test_ids = sample_submission['id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f96e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_labels = pd.read_csv(root_dir + \"training_labels.csv\", nrows=1000)\n",
    "\n",
    "# ********************\n",
    "\n",
    "# Now I shall genereate train indices, validation indices and test indices\n",
    "# Which are just the values from the 0-based indices\n",
    "train_indices, validation_indices = train_test_split(list(train_labels.index), test_size=0.33, random_state=2021)\n",
    "print(len(train_indices))\n",
    "print(len(validation_indices))\n",
    "test_indices = list(sample_submission.index)\n",
    "test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5014b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator_for_seq_model = DataGenerator( root_dir +  'train/', train_indices, train_labels, 64)\n",
    "# print(train_generator_for_seq_model)\n",
    "validation_generator_for_seq_model = DataGenerator( root_dir + 'train/', validation_indices, train_labels, 64)\n",
    "test_generator_for_seq_model = DataGenerator( root_dir + 'test/', test_indices, sample_submission, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36fad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keras_seq = Sequential()\n",
    "model_keras_seq.add(Conv1D(64, input_shape=(3, 4096), kernel_size=3, activation='relu'))\n",
    "model_keras_seq.add(BatchNormalization())\n",
    "model_keras_seq.add(Flatten())\n",
    "model_keras_seq.add(Dense(64, activation='relu'))\n",
    "model_keras_seq.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_keras_seq.compile(optimizer= Adam(lr=2e-4), loss='binary_crossentropy', metrics=['acc'])\n",
    "model_keras_seq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0baf322",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = model_keras_seq.fit_generator(generator=train_generator_for_seq_model, validation_data=validation_generator_for_seq_model, epochs = 1, workers=-1)\n",
    "# Running for 1 epoch took almost 2 and half hours.\n",
    "\n",
    "predicted_test_seq_keras = model_keras_seq.predict_generator(test_generator_for_seq_model, verbose=1)\n",
    "\n",
    "sample_submission['target'] = predicted_test_seq_keras[:len(sample_submission)]\n",
    "\n",
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
