{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91419897",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d476ec1a",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10560634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow and sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPool1D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "# To set learning rate\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9033f27e",
   "metadata": {},
   "source": [
    "## Setup variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd402fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"data/data_path.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84855949",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_paths = glob(\"D:/Projects/G2Net-Gravitational-Wave-Detection/data/train/*/*/*/*\")\n",
    "print(\"The total number of files in the training set:\", len(training_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3520477",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [path.split(\"\\\\\")[-1].split(\".\")[0] for path in training_paths]\n",
    "paths_df = pd.DataFrame({\"path\":training_paths, \"id\": ids})\n",
    "train_data = pd.merge(left=training_labels, right=paths_df, on=\"id\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd201220",
   "metadata": {},
   "source": [
    "# Modelo de https://github.com/Rtavakol/Kaggle_G2Net-Gravitational-Wave-Detection/blob/main/Gravitational_wave.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb63a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a simple sequential model with one conv layers\n",
    "model = Sequential()\n",
    "\n",
    "# step 1: 1st Convlution layer\n",
    "model.add(Conv1D(128, kernel_size = 3,activation='relu', input_shape=(3, 4096)))\n",
    "\n",
    "# step 2: Flattening\n",
    "model.add(Flatten())\n",
    "\n",
    "# step 3: Full connection \n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "# We have a binary classification, so the number of nodes would be 1\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e49ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets train our model using only 10000 time series, \n",
    "# eventually we need to use a data generator as we run out of memory if we want to \n",
    "# use all training and test datasets.\n",
    "N = 1000\n",
    "train_x  = np.zeros((N, 3, 4096))\n",
    "for i in range(N):\n",
    "    data = np.load(training_paths[i])\n",
    "    mean = np.mean(data, axis=1)\n",
    "    std = np.std(data, axis = 1)\n",
    "    data_m = [(data[i] - mean[i])/std[i] for i in range(3)]\n",
    "    train_x[i,:] = data_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d2af12",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d70b7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0ef4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = training_labels.iloc[:N, 1].values\n",
    "print(len(train_y))\n",
    "train_y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e417d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_reshaped = train_x.reshape(-1,3, 4096)\n",
    "np.shape(train_x_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a35fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_reshaped = train_y.reshape(-1, 1)\n",
    "train_y_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357275ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Keras Callbacks for learning rate\n",
    "my_callbacks_lr = [LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x, verbose=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65e0c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting CNN to training dataset\n",
    "result = model.fit(x = train_x_reshaped,\n",
    "              y = train_y_reshaped,\n",
    "              epochs = 20,\n",
    "              batch_size= 32, \n",
    "              verbose= 1, \n",
    "              callbacks= my_callbacks_lr,\n",
    "              validation_split= 0.2,\n",
    "              shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b571c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.plot(result.history['accuracy'], label = 'Accuracy')\n",
    "plt.plot(result.history['val_accuracy'], label = 'Validation Accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('Accuracy.png', dpi = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795b273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets do some experiment with this limitted 10000 samples\n",
    "# first experiment on number of filters\n",
    "n = 4 # number of try\n",
    "model = [0] * n\n",
    "filter_number = [64*(i + 1) for i in range(4)]\n",
    "for i, f in zip(range(N), filter_number):\n",
    "    # Make a simple sequential model with one conv layers\n",
    "    model[i] = Sequential()\n",
    "\n",
    "    # step 1: 1st Convlution layer\n",
    "    model[i].add(Conv1D(f, kernel_size = 3,activation='relu', input_shape=(3, 4096)))\n",
    "\n",
    "    # step 2: Flattening\n",
    "    model[i].add(Flatten())\n",
    "\n",
    "    # step 3: Full connection \n",
    "    model[i].add(Dense(64, activation='relu'))\n",
    "    # We have a binary classification, so the number of nodes would be 1\n",
    "    model[i].add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model[i].compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # Model summary\n",
    "    model[i].summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35daf8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [0] * N\n",
    "for i in range(4): \n",
    "    # Fitting CNN to training dataset\n",
    "    result[i] = model[i].fit(x = train_x_reshaped,\n",
    "              y = train_y_reshaped,\n",
    "              epochs = 20,\n",
    "              batch_size= 32, \n",
    "              verbose= 1, \n",
    "              callbacks= my_callbacks_lr,\n",
    "              validation_split= 0.2,\n",
    "              shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77cd07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "for i in range(n):\n",
    "    plt.plot(result[i].history['accuracy'], label = 'Model: {}, acc'.format(i))\n",
    "    plt.plot(result[i].history['val_accuracy'], label = 'Model: {}, val_acc'.format(i))\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('models_Accuracy.png', dpi = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303bbb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We see overfitting for 128 and 256\n",
    "# Let go with 64 filters and now do ecperiment on dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9804768",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keras_seq = Sequential()\n",
    "model_keras_seq.add(Conv1D(64, input_shape=(3, 4096), kernel_size=3, activation='relu'))\n",
    "model_keras_seq.add(BatchNormalization())\n",
    "model_keras_seq.add(Flatten())\n",
    "model_keras_seq.add(Dense(64, activation='relu'))\n",
    "model_keras_seq.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_keras_seq.compile(optimizer= Adam(lr=2e-4), loss='binary_crossentropy', metrics=['acc'])\n",
    "model_keras_seq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f63046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To feed all training data we should define a data generator as the data size is very large and our memory can\n",
    "# not handle it. So, we use a data generator to feed our model batch by batch \n",
    "# in a real time mode instead of a passive mode\n",
    "class data_generator(Sequence):\n",
    "    \n",
    "    def __init__(self, path, list_IDs, data, batch_size):\n",
    "        self.list_IDs = list_IDs\n",
    "        self.data = data\n",
    "        self.path = path\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        \n",
    "    def __len__(self):\n",
    "        len_ = int(len(self.list_IDs)/self.batch_size)\n",
    "        if len_ * self.batch_size < len(self.list_IDs):\n",
    "            len_ += 1\n",
    "        return len_\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        return X, y\n",
    "    \n",
    "    def _data_generator__data_generation(self, list_IDs_temp):\n",
    "        X = np.zeros((self.batch_size, 3, 4096))\n",
    "        y = np.zeros((self.batch_size, 1))\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            id_ = self.data.loc[ID, 'id']\n",
    "            file = id_ + '.npy'\n",
    "            path_in = '/'.join([self.path, id_[0], id_[1], id_[2]]) + '/'\n",
    "            data_array = np.load(path_in + file)\n",
    "            data_array = (data_array - data_array.mean())/data_array.std()\n",
    "            X[i, ] = data_array\n",
    "            y[i, ] = self.data.loc[ID, 'target']\n",
    "        return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24366998",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8a2ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = sample_submission['id'].values\n",
    "test_indices = list(sample_submission.index)\n",
    "\n",
    "train_ids = training_labels['id'].values\n",
    "train_y = training_labels.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06de2e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices, validation_indices = train_test_split(list(training_labels.index), test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e5491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '../Gravitational_Wave_data/train_extracted/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa58245",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = data_generator(root_dir, train_indices, training_labels, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b046a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator  = data_generator(root_dir, test_indices, training_labels, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce58060",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = data_generator(root_dir, validation_indices, sample_submission, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87592ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = model.fit_generator(generator=train_generator, validation_data=validation_generator, epochs=1, workers=-1)\n",
    "test_prediction = model.predict_generator(test_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9746862",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13380470",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b72501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b889db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['target'] = test_prediction[:len(sample_submission)]\n",
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d006ea32",
   "metadata": {},
   "source": [
    "# Modelo de https://github.com/rohan-paul/Gravitational-Wave-Detection_Kaggle_Competition/blob/main/Kaggle_NBs/1_TimeSeries_GWPy_Data_Preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6a1745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********** FOR GOOGLE DRIVE AND COLAB *****************\n",
    "\n",
    "import os \n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "\n",
    "!python -m pip install gwpy\n",
    "!pip install --upgrade --force-reinstall --no-deps gwpy\n",
    "!pip install astropy\n",
    "!pip install nnAudio\n",
    "!pip install colorama\n",
    "\n",
    "!pip install --upgrade --force-reinstall --no-deps matplotlib\n",
    "\n",
    "!pip install --force-reinstall --no-deps matplotlib==3.2.2\n",
    "# For running in Colab I have to have a previous version of matplotlib\n",
    "# This for Gihut Issue > https://github.com/gwpy/gwpy/issues/1398\n",
    "# More details are in my note in previous cell\n",
    "\n",
    "!pip install gwosc\n",
    "!pip install dqsegdb2\n",
    "!pip install ligotimegps\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from gwpy.timeseries import TimeSeries\n",
    "from gwpy.plot import Plot\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from colorama import Fore, Back, Style\n",
    "plt.style.use('ggplot')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPool1D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "import torch\n",
    "from nnAudio.Spectrogram import CQT1992v2\n",
    "\n",
    "\n",
    "from src.model.model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1135ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" First, we define the constructor to initialize the configuration of the generator.\n",
    "Note that here, we assume the path to the data is in a dataframe column.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "\n",
    "    # For this dataset the list_IDs are the value of the ids\n",
    "    # for each of the time-series file\n",
    "    # i.e. for Train data => values of column 'id' from training_labels.csv\n",
    "\n",
    "    # Also Note we have earlier defined our labels to be the below\n",
    "    # labels = pd.read_csv(root_dir + \"training_labels.csv\")\n",
    "    # and the argument \"data\" is that label here.\n",
    "    def __init__(self, path, list_IDs, data, batch_size):\n",
    "        self.path = path\n",
    "        self.list_IDs = list_IDs\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "\n",
    "    \"\"\" __len__ essentially returns the number of steps in an epoch, using the samples and the batch size.\n",
    "        Each call requests a batch index between 0 and the total number of batches, where the latter is specified in the __len__ method.\n",
    "        A common practice is to set this value to (samples / batch size)\n",
    "        so that the model sees the training samples at most once per epoch.\n",
    "        Now, when the batch corresponding to a given index is called, the generator executes the __getitem__ method to generate it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __len__(self):\n",
    "        len_ = int(len(self.list_IDs)/self.batch_size)\n",
    "        if len_ * self.batch_size < len(self.list_IDs):\n",
    "            len_ += 1\n",
    "        return len_\n",
    "\n",
    "    \"\"\"  __getitem__ method is called with the batch number as an argument to obtain a given batch of data.\n",
    "\n",
    "    \"\"\"\n",
    "    def __getitem__(self, index):\n",
    "        # get the range to to feed to keras for each epoch\n",
    "        # incrementing by +1 the bath_size\n",
    "        indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "        return X, y\n",
    "\n",
    "    \"\"\" And finally the core method which will actually produce batches of data. This private method __data_generation \"\"\"\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        # We have 5,60,000 files, each with dimension of 3 * 4096\n",
    "        X = np.zeros((self.batch_size, 3, 4096))\n",
    "        y = np.zeros((self.batch_size, 1))\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            id_ = self.data.loc[ID, \"id\"]\n",
    "            file = id_ + \".npy\"  # build the file name\n",
    "            path_in = \"/\".join([self.path, id_[0], id_[1], id_[2]]) + \"/\"\n",
    "            # there are three nesting labels inside train/ or test/\n",
    "            data_array = np.load(path_in + file)            \n",
    "            data_array = (data_array - data_array.mean())/data_array.std()\n",
    "            X[i, ] = data_array\n",
    "            y[i, ] = self.data.loc[ID, 'target']\n",
    "        # print(X)\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa33a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(root_dir +  'sample_submission.csv')\n",
    "# print(len(train_labels)) # 5,60,000\n",
    "# print(len(sample_submission)) # 2,26,000\n",
    "train_ids = train_labels['id'].values\n",
    "# train_ids # ['00000e74ad', '00001f4945', '0000661522' ... ]\n",
    "y = train_labels['target'].values\n",
    "test_ids = sample_submission['id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19943e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_labels = pd.read_csv(root_dir + \"training_labels.csv\", nrows=1000)\n",
    "\n",
    "# ********************\n",
    "\n",
    "# Now I shall genereate train indices, validation indices and test indices\n",
    "# Which are just the values from the 0-based indices\n",
    "train_indices, validation_indices = train_test_split(list(train_labels.index), test_size=0.33, random_state=2021)\n",
    "# print(len(train_indices))\n",
    "print(len(validation_indices))\n",
    "test_indices = list(sample_submission.index)\n",
    "# test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafef887",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator_for_seq_model = DataGenerator( root_dir +  'train/', train_indices, train_labels, 64)\n",
    "# print(train_generator_for_seq_model)\n",
    "validation_generator_for_seq_model = DataGenerator( root_dir + 'train/', validation_indices, train_labels, 64)\n",
    "test_generator_for_seq_model = DataGenerator( root_dir + 'test/', test_indices, sample_submission, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f634a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keras_seq = Sequential()\n",
    "model_keras_seq.add(Conv1D(64, input_shape=(3, 4096), kernel_size=3, activation='relu'))\n",
    "model_keras_seq.add(BatchNormalization())\n",
    "model_keras_seq.add(Flatten())\n",
    "model_keras_seq.add(Dense(64, activation='relu'))\n",
    "model_keras_seq.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_keras_seq.compile(optimizer= Adam(lr=2e-4), loss='binary_crossentropy', metrics=['acc'])\n",
    "model_keras_seq.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98a0936",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_keras_seq.fit_generator(generator=train_generator_for_seq_model, validation_data=validation_generator_for_seq_model, epochs = 1, workers=-1)\n",
    "# Running for 1 epoch took almost 2 and half hours.\n",
    "\n",
    "predicted_test_seq_keras = model_keras_seq.predict_generator(test_generator_for_seq_model, verbose=1)\n",
    "\n",
    "sample_submission['target'] = predicted_test_seq_keras[:len(sample_submission)]\n",
    "\n",
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8d37e8",
   "metadata": {},
   "source": [
    "# Modelo de https://github.com/PraveenThakkannavar/G2Net-Gravitational-Wave-Detection/blob/main/SIMPLE_CNN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d59949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Sequential model\n",
    "model_cnn = Sequential(name='CNN_model')\n",
    "\n",
    "# Add the first Convoluted2D layer w/ input_shape & MaxPooling2D layer followed by that\n",
    "model_cnn.add(Conv2D(filters=16,\n",
    "                     kernel_size=3,\n",
    "                     input_shape=input_shape,\n",
    "                     activation='relu',\n",
    "                     name='Conv_01'))\n",
    "model_cnn.add(MaxPooling2D(pool_size=2, name='Pool_01'))\n",
    "\n",
    "# Second pair of Conv1D and MaxPooling1D layers\n",
    "model_cnn.add(Conv2D(filters=32,\n",
    "                     kernel_size=3,\n",
    "                     input_shape=input_shape,\n",
    "                     activation='relu',\n",
    "                     name='Conv_02'))\n",
    "model_cnn.add(MaxPooling2D(pool_size=2, name='Pool_02'))\n",
    "\n",
    "# Third pair of Conv1D and MaxPooling1D layers\n",
    "model_cnn.add(Conv2D(filters=64,\n",
    "                     kernel_size=3,\n",
    "                     input_shape=input_shape,\n",
    "                     activation='relu',\n",
    "                     name='Conv_03'))\n",
    "model_cnn.add(MaxPooling2D(pool_size=2, name='Pool_03'))\n",
    "\n",
    "# Add the Flatten layer\n",
    "model_cnn.add(Flatten(name='Flatten'))\n",
    "\n",
    "# Add the Dense layers\n",
    "model_cnn.add(Dense(units=512,\n",
    "                activation='relu',\n",
    "                name='Dense_01'))\n",
    "model_cnn.add(Dense(units=64,\n",
    "                activation='relu',\n",
    "                name='Dense_02'))\n",
    "\n",
    "# Add the final Output layer\n",
    "model_cnn.add(Dense(1, activation='sigmoid', name='Output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6475ea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de0418",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=[[AUC(), 'accuracy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4a45b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data\n",
    "history_cnn = model_cnn.fit(x=train_dataset,\n",
    "                            epochs=3,\n",
    "                            validation_data=valid_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
